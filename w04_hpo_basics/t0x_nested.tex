\input{t00_template.tex}
\subtitle{Nested Resampling}

\begin{document}

\maketitle


%----------------------------------------------------------------------
%----------------------------------------------------------------------

\begin{frame}[allowframebreaks,containsverbatim]{Nested Crossvalidation}
Selecting the best model from a set of potential candidates (e.g., different classes of learners, different hyperparameter settings, different feature sets, different preprocessing, ....) is an important part of most  machine learning problems.

\begin{blocki}{Problem}
    \item We cannot evaluate our finally selected learner on the same resampling splits that we have used to perform model selection for it, e.g., to tune its hyperparameters.
    \item By repeatedly evaluating the learner on the same test set, or the same CV splits, information
      about the test set leaks into our evaluation.
    \item Danger of overfitting to the resampling splits / overtuning!
    \item The final performance estimate will be optimistically biased.
    \item One could also see this as a problem similar to multiple testing.
\end{blocki}

\framebreak

\begin{itemize}
\item All parts of the model building (including model selection, preprocessing) should be embedded in the model-finding process \textbf{on the training data}.
\item The test set should only be touched once, so we have no way of \textit{cheating}. The test dataset is only used once \emph{after} a model is completely trained, after deciding for example on specific hyper-parameters.

Only if we do this, the performance estimates we obtained from the test set are \textbf{unbiased estimates} of the true performance.

\item For steps that themselves require resampling (e.g., hyperparameter tuning) this results
  in \textbf{nested resampling}, i.e., resampling strategies for both.
  \begin{itemize}
  \item tuning: an inner resampling loop to find what works best based on training data
  \item evaluation: an outer resampling loop to evaluate on data not used for tuning to get honest estimates of the expected performance on new data
  \end{itemize}
\end{itemize}

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.4\textwidth}
\hspace*{-0.3cm}
Simplest method to achieve this: a 3-way split
\begin{itemize}
\item During tuning, a learner is trained on the \textbf{training set},
  evaluated on the  \textbf{validation set}.
\item After the best model configuration $\finconf$ is selected, we re-train on the joint (training+validation) set and evaluate the model's performance on the \textbf{test set}.
\end{itemize}

\column{0.5\textwidth}
\hspace*{-0.7cm}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.2\textwidth]{images/train_valid_test.pdf}
\end{center}
\end{columns}
\framebreak


\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.4\textwidth}
\hspace*{-0.3cm}
\begin{itemize}
\item Effectively, the tuning step is now simply part of a more complex training procedure.
\item We could see this as removing the hyperparameters from the inputs of the algorithm and making it \textit{self-tuning}.
\end{itemize}

\column{0.5\textwidth}
\hspace*{-0.7cm}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.2\textwidth]{images/autotune_in_model_fit.pdf}
\end{center}
\end{columns}

\framebreak

More precisely: the combined training \& validation set is actually the training set for the \textit{self-tuning} endowed algorithm.

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=1.3\textwidth]{images/train_valid_test.pdf}
\column{0.45\textwidth}
\hspace*{-0.7cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
\includegraphics[width=1.3\textwidth]{images/autotune_in_model_fit.pdf}
\end{columns}

\framebreak

Just like we can generalize holdout splitting to resampling to get more reliable estimates of the predictive performance, we can generalize the training/validation/test approach to \textbf{nested resampling}.
\vskip 2em
This results in two resampling loops, whereas the resampling for the tuning is nested in the resampling for the outer evaluation, i.e., one resampling strategy for  tuning and another for outer evaluation.
% \vskip 2em
% Let's look at this for a simple example: say we want to find out whether a logistic regression model or a $k$-NN classifier performs better on some data set.

% \framebreak
% FIGURE SOURCE: https://docs.google.com/presentation/d/1W9nPHJa39Tkzzp37ZeRsEaSZjXMdFjH-rmIpimOKZKA/edit#slide=id.g61fd3b961b_0_1260
% \begin{center}\includegraphics[page = 1, width = \textwidth]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 2]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 3]{images/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 4]{images/Outer_CV.pdf}\end{center}

% \framebreak

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
Assume we want to tune over a set of candidate HP configurations $\conf_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer loop. The outer loop is visualized as the light green and dark green parts.

\column{0.5\textwidth}
\vspace*{-0.3cm}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = \textwidth]{images/Nested_Resampling.png}\end{center}
\end{columns}

\framebreak

\vspace{1cm}
\begin{columns}[c, onlytextwidth]
\column{0.45\textwidth}
\hspace*{-0.3cm}
\begin{footnotesize}
In each iteration of the outer loop we:
\begin{itemize}
\item Split off the light green testing data
\item Run the tuner on the dark green part of the data, e.g.,
  evaluate each $\conf_i$ through fourfold CV on the dark green part
\item Return the winning $\finconf$ that performed best on the grey inner test sets
\item Re-train the model on the full outer dark green train set
\item Evaluate it on the outer light green test set
\end{itemize}
\end{footnotesize}

\column{0.5\textwidth}
\vspace*{-0.3cm}
% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = \textwidth]{images/Nested_Resampling.png}\end{center}
\end{columns}

\begin{footnotesize}
The error estimates on the outer samples (light green) are unbiased because this data was strictly excluded from the model-building process of the model that was tested on.
\end{footnotesize}

\end{frame}

\end{document}
