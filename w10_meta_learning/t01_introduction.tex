\input{../latex_main/main.tex}
\usepackage{multimedia}

\title[Meta-Learning]{AutoML: Meta-Learning} 
\subtitle{Introduction}
\author[Joaquin Vanschoren]{Bernd Bischl \and Frank Hutter \and Lars Kotthoff\newline \and Marius Lindauer \and \underline{Joaquin Vanschoren}}
\institute{}
\date{}

\begin{document}
\maketitle

%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
% \begin{frame}
% \frametitle{Table of Contents}
% \tableofcontents
% \end{frame}
%---------------------------------------------------------


% \section{First section}

%---------------------------------------------------------
%Changing visivility of the text
\begin{frame}
\frametitle{Intro: humans can easily learn from a single example}
thanks to years of learning (and eons of evolution) 

% \movie[width=8cm,height=4.5cm]{train}{video.mov}
\includegraphics[height=6cm]{image/img162621.jpg}

\end{frame}

%---------------------------------------------------------
\begin{frame}{Can a computer learn from a single example?}
\includegraphics[height=5cm]{image/img163023.jpg}

\hspace*{\fill} \
\hspace*{\fill} \
\pause
\centerline{\textbf{That won’t work :) Humans also don’t start from scratch.}}
\end{frame}


\begin{frame}{Transfer learning?}
\includegraphics[height=6.5cm]{image/img163705.jpg}

\pause
\small\centerline{\textbf{A single source task (e.g. ImageNet) may not generalize well to the test task.}}
\end{frame}

\begin{frame}{Meta-learning}{Learn over a series (or distribution) of many different tasks/episodes}
\centerline{\textit{\textcolor{red}{Inductive bias (or prior)}: learn \textcolor{red}{assumptions} that you can to transfer to new tasks}}
\centerline{\textit{Prepare yourself to learn new things faster}}
\centering\includegraphics[height=4.9cm]{image/img182859.jpg}

\pause
\leavevmode\hphantom{ }

\small\centerline{Closely related to continual learning, online learning, multi-task learning}
\end{frame}

\begin{frame}{Meta-learning}{Learn over a series (or distribution) of many different tasks/episodes}
  \centerline{\textit{\textcolor{red}{Inductive bias (or prior)}: learn \textcolor{red}{assumptions} that you can to transfer to new tasks}}
  \centerline{\textit{Prepare yourself to learn new things faster}}
  \centering\includegraphics[height=4.9cm]{image/img182859.jpg}
  
  \leavevmode\hphantom{ }
  
  \small\centerline{Useful in many real-life situations: rare events, test-time constraints, data collection costs, privacy issues,...}
  \end{frame}
  

\begin{frame}{Meta-learning for AutoML?}
  \centerline{AutoML systems should not start from scratch, they should learn across tasks}
  \centerline{Human ML experts also get better over time}

  \centering\includegraphics[height=4.5cm]{image/imgnew1.png}
  
  \begin{itemize}
    \item Current AutoML systems require humans to hard-code a set of assumptions
    \item What if AutoML systems could learn these by themselves?
  \end{itemize}

\end{frame}

\begin{frame}{Meta-learning for AutoML?}
  \centerline{AutoML systems should not start from scratch, they should learn across tasks}
  \centerline{Human ML experts also get better over time}

  \centering\includegraphics[height=4.5cm]{image/imgnew1.png}
  
  \begin{itemize}
    \item Meta-learning can make AutoML much more efficient
    \item Vice-versa: AutoML can make meta-learning more robust
  \end{itemize}

\end{frame}

\begin{frame}{Inspired by human learning}
\centerline{We don’t transfer from a single source task, we learn across many, many tasks}
\centerline{We have a ‘drive’ to explore new, challenging, but doable, fun tasks}
 
\centering\includegraphics[height=5cm]{image/img184210.jpg}

\end{frame}

\begin{frame}{Human-like Learning***}
\centerline{humans learn across tasks: less trial-and-error, less data, less compute}
\centerline{\textcolor{orange}{new tasks should be related to experience (doable, fun, interesting?)}}
 
\centering\includegraphics[height=5cm]{image/img184546.jpg}

\centerline{\textit{key aspects of fast learning: compositionality, causality, \underline{learning to learn}}}
\vspace{0.3cm}
\raggedright{\footnotesize{\textit{
\href{https://cims.nyu.edu/~brenden/papers/LakeEtAl2017BBS.pdf}{Lake et al. (2017) Building machines that learn and think like people.}}
}}

\end{frame}

\begin{frame}{Inductive bias (in language)}
\centerline{\textit{which assumptions do we make?}}
\centering\includegraphics[height=6.2cm]{image/img185829.jpg}

\end{frame}

\begin{frame}{Inductive bias (in language)}
\centerline{\textit{which assumptions do we make?}}
\centering\includegraphics[height=6cm]{image/img185953.jpg}

\end{frame}

\begin{frame}{Inductive bias (in language)}
\centerline{\textit{which assumptions do we make?}}
\centering\includegraphics[height=5.9cm]{image/img190213.jpg}

\end{frame}

\begin{frame}{Inductive bias (in language)}
\centerline{\textit{which assumptions do we make?}}
\centering\includegraphics[height=5.9cm]{image/img190532.jpg}

\end{frame}

\begin{frame}{Inductive bias (in language)}
\centerline{\textit{which assumptions do we make?}}
\centering\includegraphics[height=5.9cm]{image/img190839.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img191412.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img191823.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img191448.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img191531.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192202.jpg}

\end{frame}

\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192229.jpg}

\end{frame}
\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192318.jpg}

\end{frame}
\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192345.jpg}

\end{frame}
\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192403.jpg}

\end{frame}
\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192435.jpg}

\end{frame}
\begin{frame}{What if there is no training data?}
\centerline{\textit{we can still solve problems by making assumptions}}
\centering\includegraphics[height=5.5cm]{image/img192503.jpg}
\pause
\leavevmode\hphantom{ }

\small\centerline{Humans \textit{assume} that words have consistent meanings and follow input/output constraints}
\pause
\small\centerline{\textbf{These assumptions \textcolor{red}{(inductive biases)} are necessary for learning quickly}}
\end{frame}

\begin{frame}{Meta-learning inductive biases}
\centerline{\textit{Capture \textcolor{red}{useful assumptions} from the data - that can often not be easily expressed}}
\centering\includegraphics[height=6cm]{image/img193434.jpg}

\vspace{0.2cm}
\raggedright{\footnotesize{\textit{
\href{https://arxiv.org/abs/1906.05381}{Lake (2019) Compositional generalization through meta sequence-to-sequence learning.}}
}}
\end{frame}

\begin{frame}{Meta-learning goal}
\centerline{learn \textit{minimal} inductive biases from prior tasks instead of constructing manual ones }
\centerline{should still generalize well (otherwise you meta-overfit)}
\centering\includegraphics[height=3.5cm]{image/img193722.jpg}


\textbf{Inductive bias:} any assumptions added to training data to learn more effectively. E.g:
\begin{itemize}
  \item Instead of \textcolor{red}{general model architectures}, \textcolor{blue}{learn better architectures (and hyperparameters)}
  \item Instead of \textcolor{red}{starting from random weights}, \textcolor{blue}{learn good initial weights}
  \item Instead of \textcolor{red}{standard loss/reward function}, \textcolor{blue}{learn a better loss/reward function}
\end{itemize}
\end{frame}

\begin{frame}{What can we learn to learn?}
\centerline{\textit{3 pillars}}
\centering\includegraphics[height=5.5cm]{image/imgnew2.png}

\vspace{0.7cm}
\raggedright{\footnotesize{\textit{
\href{https://arxiv.org/abs/1905.10985}{Clune (2019) AI-GAs: AI-generating algorithms.}}
}}
\end{frame}

\end{document}