\input{../latex_main/main.tex}

\title{Speedup Techniques for Hyperparameter Optimization}
\subtitle{Predicting Learning Curves}
\author[Frank Hutter]{Bernd Bischl \and \underline{Frank Hutter} \and Lars Kotthoff\newline \and Marius Lindauer \and Joaquin Vanschoren}
\institute{}
\date{}



\begin{document}
\maketitle

%----------------------------------------------------------------------

\begin{frame}{Learning Curves}
\begin{center}
\includegraphics[width=0.4\linewidth, keepaspectratio=true]{images/intro/graybox_optimization.png}
\end{center}

\begin{figure}
   \centering
   \includegraphics[width=0.34\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_domhan.png}
   \caption{Exemplary learning curves of training deep neural networks}
\end{figure}
\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}{Learning Curve Predictions}
\onslide<1->
\begin{figure}
    \centering
    \only <1>{\includegraphics[width=0.5\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_questionmark.png}}
    \only <2->{\includegraphics[width=0.5\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_single_pred.jpg}}
    
\end{figure}


\begin{enumerate}
  \item Observe learning curve for the first $n$ steps (here $n=250$)
  \pause
  \item \alert{Extrapolation}: fit parametric model on partial learning curve to predict remaining learning curve
  \pause
  \begin{itemize}
      \item Various models can be used (see following slides) 
  \end{itemize}
  
 % Which model to use? E.g.,
 % \begin{itemize}
%	\item Parametric density models: give table with equations
%	\item Neural network with learning curve layer
%	\item Recurrent neural network
%%    \item Good model depends on shape of curve $\to$ e.$\,$g., depends on optimizer  
%%    \item[$\leadsto$] combination of several models
%  \end{itemize}
  
\end{enumerate}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}{Learning Curves: Early Termination}

\centering
\includegraphics[width=0.75\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_dec.png}

$\rightarrow$ need for \alert{probabilistic predictions / quantification of uncertainty}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}{Learning Curves: Early Termination}

\centering
\includegraphics[width=0.75\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_dec2.png}

$\rightarrow$ need for \alert{probabilistic predictions / quantification of uncertainty}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}{Parametric Learning Curves \litw{\href{https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/view/11468/11222}{Domhan et al. 2015}}}

\vspace*{-0.25cm}
\begin{itemize}
	\item Use a parametric model $f_k$ with parameters $\boldsymbol{\theta}$ to model performance at step $t$ as:
	\alert{$y_t = f_k(t|\boldsymbol{\theta}) + \epsilon$}, with $\epsilon \sim \mathcal{N}(0, \sigma^2)$.
\pause
	\item Linear combination of $K=11$ parametric types of models:
	\alert{$f_{comb}(t|\bm{\xi}) = \sum_{k=1}^K w_k f_k(t|\boldsymbol{\theta}_k)$},
where $\bm{\xi} = (w_1, \dots, w_{K}, \boldsymbol{\theta}_1, \dots, \boldsymbol{\theta}_{K}, \sigma^2)$
%	\item MSc Thesis in my group, 2015
\end{itemize}
\pause
\begin{center}
\includegraphics[width=0.9\textwidth]{../w07_hpo_speedup/images/learningcurve/Domhan_types_of_curves.png}\\
\scriptsize{$K=11$ parametric families for modelling learning curves}
\end{center}

\pause
\begin{itemize}
	\item Use Markov Chain Monte Carlo sampling of $\bm{\xi}$ to obtain uncertainties
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\begin{frame}{Predictive Termination}
\vspace*{-0.2cm}
{
\begin{center}
\includegraphics[width=0.9\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_tuning.jpg}

All learning curves vs. learning curves with early termination
\end{center}
}




\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}{Predictive Termination}

{
\begin{center}
\includegraphics[width=0.5\textwidth]{../w07_hpo_speedup/images/learningcurve/learning_curve_tuning.jpg}

All learning curves vs. learning curves with early termination
\end{center}
}
\begin{itemize}
	\item Disadvantages of this model?
\pause
	\begin{itemize}
		\item Relies on manually-selected parametric families of curves
		\item Does not take into account hyperparameters used 
		\begin{itemize}
			\item[$\rightarrow$] can't learn across hyperparameters
		\end{itemize}
%		\item Does not learn across curves; simply extrapolates one at a time
%		\item Cannot quickly integrate new information from extending the curve
	\end{itemize}
\end{itemize}

\end{frame}

%-----------------------------------------------------------------------
\begin{frame}{LC-Net \litw{\href{https://openreview.net/pdf?id=S11KBYclx}{Klein et al. 2017}}}

\vspace*{-0.25em}
{
	\rightimage[.4]{../w07_hpo_speedup/images/learningcurve/LC-Net-network.png}
	\begin{itemize}
		\item \goleft[.45]{Make a layer out of the parametric learning curves by Domhan et al.}
		\item \goleft[.45]{Also support hyperparameters as inputs (in the figure denoted by $x_1, \dots, x_d$)}
%		\item \goleft[.45]{Work by my Phd student Aaron Klein and postdoc Stefan Falkner}
	\end{itemize}
}

\pause
\begin{itemize}
	\item Disadvantages of this model?
	\pause
	\begin{itemize}
		\item \goleft[.45]{Relies on manually-selected parametric families of curves}
		\item \goleft[.45]{Cannot quickly integrate new information from extending the current curve \\ (or from new runs)}
\pause
		\item Also, the model is very hard to train	
	\end{itemize}
\end{itemize}
\end{frame}
%-----------------------------------------------------------------------




%-----------------------------------------------------------------------
\begin{frame}{Sequence Models {\smaller{(e.g., Bayesian RNN)}} \litw{\href{https://www.automl.org/wp-content/uploads/2019/06/automlws2019_Paper24.pdf}{Gargiani et al. 2019}}}

	\begin{itemize}
		\item Learning curves are \alert{sequences}
		\begin{itemize}
			\item Previous models don't treat them like this
			\item We can use an RNN (in particular, an LSTM) to predict the next value from a given sequence
			\item We can use variational dropout to obtain uncertainty estimates:
\pause
		\end{itemize}
	\end{itemize}

\begin{center}
	\includegraphics[width=0.45\textwidth]{../w07_hpo_speedup/images/learningcurve/Gargiani-MNIST-extrapolations_1.png}\\
	\includegraphics[width=0.45\textwidth]{../w07_hpo_speedup/images/learningcurve/Gargiani-MNIST-extrapolations_2.png}\\
\end{center}	

\pause
	Note: we can also use a simpler model
	\begin{itemize}
		\item E.g., a random forest to map from a fixed-size window to the next value
	\end{itemize}

	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
%\begin{frame}{Sequence Models {\smaller{(e.g., Bayesian RNN)}}}
%
%	\myit{
%		\item Note: we can also use a simpler model
%		\myit{
%			\item E.g., a random forest to map from a fixed-size window to the next value
%		}
%	}
%\begin{center}
%	\includegraphics[width=0.9\textwidth]{../w07_hpo_speedup/images/learningcurve/Gargiani-MNIST-extrapolation-quality-based-on-different-sized-prefixes.png}
%\end{center}	
%\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\begin{frame}{Compare: Baker et al, 2017 \litw{\href{https://openreview.net/pdf?id=BJypUGZ0Z}{Baker et al. 2018}}}

	\begin{itemize}
		\item Idea: map from configurations (including architectural hyperparameters) 
		and partial learning curves to the final performance

		\item Advantages
		\begin{itemize}
			\item \alert{Much simpler idea} than all the approaches just discussed:\\ no need to model the entire learning curve
			\item \alert{Much easier to implement}
		\end{itemize}
		\item Disadvantage? \pause
		\alert{$\rightarrow$ requires many (e.g., 100) fully-evaluated learning curves as training data}
		\begin{itemize}
			\item After 100 full function evaluations we want to be pretty much converged in practice
			\item But definitely helpful for speeding up RL
			
		\end{itemize}
	\end{itemize}
%Describe simple idea, and how that works extremely well when you have a budget of 10.000 of function evaluations.

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------

\begin{frame}{Freeze-Thaw Bayesian Optimization \litw{\href{https://arxiv.org/pdf/1406.3896.pdf}{Swersky et al. 2014}}}
\begin{itemize}

	\item Use a Gaussian process with inputs $\conf$ and $t$; special kernel for $t$
	\item For $N$ configurations and $T$ epochs each: $O(N^3 t^3)$ $\rightarrow$ approximation
	\item Iteratively: either extend existing configuration or try new one
\pause
	\item Result for probabilistic matrix factorization:
	\includegraphics[width=0.8\textwidth]{../w07_hpo_speedup/images/learningcurve/FTBO.png}
\pause
	\item Unfortunately, no results for DNNs; no code available
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------




%\myframe{Possible extension of LC models (under development)}{
%	\myit{
%		\item We could keep track of additional information to feed to our model for better predictions
%		\myit{
%			\item E.g., training \& validation cross-entropy loss \& accuracy
%			\myit{
%				\item Instead of only validation accuracy
%			}
%			\item E.g., split cross-entropy into data-dependent \& weight dependent parts
%			\item E.g., keep track of gradient norms, activation statistics, \ldots
%		}
%\bigskip
%        \item Information about learning rate (\& weight decay) at each step
%%		\begin{itemize}
%%		    \item Only Predictive termination leads to a practically usable algorithm
%%		\end{itemize}
%	}
%}

%-----------------------------------------------------------------------
\begin{frame}[c]{Questions to Answer for Yourself / Discuss with Friends}

\begin{itemize}
    \item \alert{Repetition.} List all learning curve prediction methods you recall, along with their pros and cons.
\medskip
    \item \alert{Discussion.} Could predictive termination cut off evaluations early that would turn out to be the best?
\medskip
    \item \alert{Discussion.} How would you determine a learning curve prediction method's own hyperparameters (such as the 5\% for early learning curve termination), in practice?
\medskip
    \item \alert{Discussion.} How could we exploit additional side information we gain about the learning curve, such as, e.g., statistics for the size of the gradients and activations over time?
\end{itemize}

\end{frame}
\end{document}